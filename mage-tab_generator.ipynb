{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HCA Spreadsheet - SCEA MAGE-TAB Converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission information\n",
    "\n",
    "Input here the accession number for the submission, the curators' initials, and the work directory where the csv files are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accession_index = 11\n",
    "curators = [\"AD\", \"JFG\"]\n",
    "work_dir = \"E-HCAD-11_HumanColonicMesenchymeIBD\"\n",
    "\n",
    "force_project_uuid = None\n",
    "\n",
    "# HumanColonicMesenchymeIBD\n",
    "force_project_uuid = \"f8aa201c-4ff1-45a4-890e-840d63459ca2\"\n",
    "\n",
    "# KidneySingleCellAtlas\n",
    "# force_project_uuid = \"abe1a013-af7a-45ed-8c26-f3793c24a1f4\"\n",
    "\n",
    "# Reprogrammed_Dendritic_Cells\n",
    "# force_project_uuid = \"116965f3-f094-4769-9d28-ae675c1b569c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accession = f\"E-HCAD-{accession_index}\"\n",
    "protocol_accession = f\"HCAD{accession_index}\"\n",
    "idf_file_name = f\"{accession}.idf.txt\"\n",
    "sdrf_file_name = f\"{accession}.sdrf.txt\"\n",
    "fill_this_label = \"<FILL THIS>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HCA Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function convert_to_snakecase\n",
    "\n",
    "Converts a label sheet to snake_case removing \" - \" and whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_snakecase(label):\n",
    "    return re.sub(r'(\\s-\\s)|\\s', '_', label).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to load spreadsheets\n",
    "\n",
    "Load all csv from a directory into a dict with their names in snake_case as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from os.path import basename, splitext\n",
    "\n",
    "def get_all_spreadsheets(work_dir):\n",
    "    file_names = glob.glob(f\"{work_dir}/[!big_table.csv]*.csv\")\n",
    "    \n",
    "    spreadsheets = {}\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        spreadsheets[convert_to_snakecase(splitext(basename(file_name))[0])] = file_name\n",
    "\n",
    "    return spreadsheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spreadsheets = get_all_spreadsheets(work_dir)\n",
    "\n",
    "for name, file_name in spreadsheets.items():\n",
    "    spreadsheets[name] = pd.read_csv(file_name, header=0, sep=\";\", skiprows=[0,1,2,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Big Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge sequence files with cell suspensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table = spreadsheets['cell_suspension'].merge(\n",
    "    spreadsheets['sequence_file'],\n",
    "    how=\"outer\",\n",
    "    on=\"cell_suspension.biomaterial_core.biomaterial_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge specimens into big table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table = spreadsheets['specimen_from_organism'].merge(\n",
    "    big_table,\n",
    "    how=\"outer\",\n",
    "    on=\"specimen_from_organism.biomaterial_core.biomaterial_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge donor organisms into big table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table = spreadsheets['donor_organism'].merge(\n",
    "    big_table,\n",
    "    how=\"outer\",\n",
    "    on=\"donor_organism.biomaterial_core.biomaterial_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge library preparation and sequencing protocols into big table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table = spreadsheets['library_preparation_protocol'].merge(\n",
    "    big_table,\n",
    "    how=\"outer\",\n",
    "    on=\"library_preparation_protocol.protocol_core.protocol_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table = spreadsheets['sequencing_protocol'].merge(\n",
    "    big_table,\n",
    "    how=\"outer\",\n",
    "    on=\"sequencing_protocol.protocol_core.protocol_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the two rows for each read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table_read1 = big_table.loc[big_table['sequence_file.read_index'] == 'read1']\n",
    "big_table_read2 = big_table.loc[big_table['sequence_file.read_index'] == 'read2']\n",
    "\n",
    "big_table_read2_short = big_table_read2[[\n",
    "    'cell_suspension.biomaterial_core.biomaterial_id',\n",
    "    'sequence_file.file_core.file_name',\n",
    "    'sequence_file.read_length',\n",
    "    'sequence_file.lane_index',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table_joined = big_table_read1.merge(\n",
    "    big_table_read2_short,\n",
    "    on=['cell_suspension.biomaterial_core.biomaterial_id', 'sequence_file.lane_index'],\n",
    "    suffixes=(\"_read1\", \"_read2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge index rows for each read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table = big_table.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table_index1 = big_table.loc[big_table['sequence_file.read_index'] == 'index1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table_index1_short = big_table_index1[[\n",
    "    'cell_suspension.biomaterial_core.biomaterial_id',\n",
    "    'sequence_file.file_core.file_name',\n",
    "    'sequence_file.read_length',\n",
    "    'sequence_file.lane_index',\n",
    "]]\n",
    "\n",
    "big_table_index1_short.columns = [f\"{x}_index1\" for x in big_table_index1_short.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table_joined2 = big_table_joined.merge(\n",
    "    big_table_index1_short,\n",
    "    left_on=['cell_suspension.biomaterial_core.biomaterial_id', 'sequence_file.lane_index'],\n",
    "    right_on=[\"cell_suspension.biomaterial_core.biomaterial_id_index1\", 'sequence_file.lane_index_index1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table_joined2.reset_index(inplace=True)\n",
    "big_table_joined2 = big_table_joined2.rename(columns={'sequence_file.file_core.file_name': 'sequence_file.file_core.file_name_read1'})\n",
    "big_table_joined_sorted = big_table_joined2.reindex(sorted(big_table_joined.columns), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Big Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table_joined_sorted = big_table_joined_sorted.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table_joined_sorted.to_csv(f\"{work_dir}/big_table.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table = big_table_joined_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_type_map = {\n",
    "    'collection_protocol': \"sample collection protocol\",\n",
    "    'dissociation_protocol': \"enrichment protocol\",\n",
    "    '??????????????????????': \"nucleic acid extraction protocol\",\n",
    "    'enrichment_protocol': \"enrichment_protocol\",\n",
    "    'library_preparation_protocol': \"nucleic acid library construction protocol\",\n",
    "    'sequencing_protocol': \"nucleic acid sequencing protocol\",\n",
    "}\n",
    "\n",
    "protocol_order = [\n",
    "    'collection_protocol',\n",
    "    'dissociation_protocol',\n",
    "    'enrichment_protocol',\n",
    "    'library_preparation_protocol',\n",
    "    'sequencing_protocol',\n",
    "]\n",
    "\n",
    "protocol_columns = [\n",
    "    ('collection_protocol', 'collection_protocol.protocol_core.protocol_id'),\n",
    "    ('library_preparation_protocol', 'library_preparation_protocol.protocol_core.protocol_id'),\n",
    "    ('sequencing_protocol', 'sequencing_protocol.protocol_core.protocol_id'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (protocol_type, _) in protocol_columns:\n",
    "    spreadsheets[protocol_type] = spreadsheets[protocol_type].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_proto_to_id(protocol_name):\n",
    "    for _, proto in protocol_map.items():\n",
    "        if protocol_name in proto:\n",
    "            return proto.get(protocol_name)['id']\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract lists of protocols\n",
    "\n",
    "Below are some helpers to convert lists in HCA spreadsheets (items are separated with two pipes `||`) to python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitlist(list_):\n",
    "    split_data = []\n",
    "    try:\n",
    "         split_data = list_.split('||')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_multiprotocols(df, proto_column):\n",
    "    proto_series = df[proto_column].apply(splitlist)\n",
    "    proto_df = pd.DataFrame(proto_series.values.tolist())\n",
    "    proto_df_columns = [f'{proto_column}_{y}' for y in range(len(proto_df.columns))]\n",
    "    proto_df.columns = proto_df_columns\n",
    "    proto_df[f'{proto_column}_count'] = proto_series.str.len()\n",
    "    proto_df[f'{proto_column}_list'] = proto_series\n",
    "    \n",
    "    return (proto_df, proto_df_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extracts the lists from protocol types which can have more than one instance and creates extra columns in the Big Table for each of the items, as well as the count and the python-style list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprotocols = [\n",
    "    ('dissociation_protocol', 'dissociation_protocol.protocol_core.protocol_id'),\n",
    "    ('enrichment_protocol', 'enrichment_protocol.protocol_core.protocol_id'),\n",
    "]\n",
    "\n",
    "for (protocol_type, protocol_field) in multiprotocols:\n",
    "    if spreadsheets.get(protocol_type) is not None:\n",
    "        spreadsheets[protocol_type] = spreadsheets[protocol_type].fillna('')\n",
    "        proto_df, proto_df_columns = split_multiprotocols(big_table, protocol_field)\n",
    "        for proto_column in proto_df_columns:\n",
    "            protocol_columns.append( (protocol_type, proto_column) )\n",
    "\n",
    "    big_table = big_table.merge(proto_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create protocol SCEA ids and map to HCA ids\n",
    "\n",
    "First, we prepare an ID minter for the protocols following SCEA MAGE-TAB standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_id_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mint_proto_id():\n",
    "    global protocol_id_counter\n",
    "    protocol_id_counter += 1\n",
    "    return f\"P-{protocol_accession}-{protocol_id_counter}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, protocol map is created: a dict containing types of protocols, and inside each, a map from HCA ids to SCEA ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "protocol_map = {x: {} for x in protocol_order}\n",
    "\n",
    "for proto_type in protocol_order:\n",
    "    for (ptype, proto_column) in protocol_columns:\n",
    "        if ptype == proto_type:\n",
    "            new_protos = pd.unique(big_table[proto_column]).tolist()\n",
    "            protocol_map[proto_type].update({proto: {\n",
    "                'id': mint_proto_id()\n",
    "            } for proto in new_protos if proto is not None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more fields will be needed, like the description and the hardware used in some protocols, so here's a function to extract info from the protocols spreadsheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_protocol_info(column_to_extract, to_key, for_protocols = protocol_order):\n",
    "    for proto_type, proto_list in protocol_map.items():\n",
    "        if proto_type in for_protocols:\n",
    "            for proto_name, proto in proto_list.items():\n",
    "                extracted_data = spreadsheets[proto_type].loc[spreadsheets[proto_type][f'{proto_type}.protocol_core.protocol_id'] == proto_name][f'{proto_type}.{column_to_extract}'].tolist()\n",
    "\n",
    "                if len(extracted_data):\n",
    "                    proto[to_key] = extracted_data[0]\n",
    "                else:\n",
    "                    proto[to_key] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that function, we get the description for all protocol types, and the hardware for sequencing protocols into the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_protocol_info(f\"protocol_core.protocol_description\", \"description\")\n",
    "extract_protocol_info(f\"instrument_manufacturer_model.ontology_label\", \"hardware\", [\"sequencing_protocol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the MAGE-TAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDF File\n",
    "\n",
    "Python does not allow backslashes in f-strings, so we assign `\\t` to `tab` and use that instead of a literal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = \"\\t\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getter function for fields in spreadsheets. Will sanitize newlines into spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(sheet, col_name, func=lambda x: x):\n",
    "    data = None\n",
    "    \n",
    "    try:\n",
    "        data = tab.join(func(p) for p in list(spreadsheets[sheet][col_name].fillna(''))).replace('\\n', ' ')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting dates from the humancellatlas API, as they are not in the spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import dateutil.parser\n",
    "\n",
    "project_uuid = g(\"project\", \"project.uuid\") if force_project_uuid is None else force_project_uuid\n",
    "submission_date = fill_this_label\n",
    "last_update_date = fill_this_label\n",
    "geo_accessions = []\n",
    "\n",
    "if project_uuid:\n",
    "    project_url = f\"https://api.ingest.data.humancellatlas.org/projects/search/findByUuid?uuid={project_uuid}\"\n",
    "    project_response = requests.get(project_url)\n",
    "    \n",
    "    if project_response.status_code == 200:\n",
    "        project_data = project_response.json()\n",
    "\n",
    "        submission_date = dateutil.parser.isoparse(project_data['submissionDate']).strftime(\"%Y-%m-%d\")\n",
    "        last_update_date = dateutil.parser.isoparse(project_data['updateDate']).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        geo_accessions = project_data['content'].get('geo_series_accessions', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_file_contents = f\"\"\"\\\n",
    "MAGE-TAB Version\\t1.1\n",
    "Investigation Title\\t{g(\"project\", \"project.project_core.project_title\")}\n",
    "Comment[Submitted Name]\\t{g(\"project\", \"project.project_core.project_short_name\")}\n",
    "Experiment Description\\t{g(\"project\", \"project.project_core.project_description\")}\n",
    "Public Release Date\\t{last_update_date}\n",
    "Person First Name\\t{g(\"project_contributors\", \"project.contributors.name\", lambda x: x.split(',')[0])}\n",
    "Person Last Name\\t{g(\"project_contributors\", \"project.contributors.name\", lambda x: x.split(',')[2])}\n",
    "Person Mid Initials\\t{g(\"project_contributors\", \"project.contributors.name\", lambda x: x.split(',')[1])}\n",
    "Person Email\\t{g(\"project_contributors\", \"project.contributors.email\")}\n",
    "Person Affiliation\\t{g(\"project_contributors\", \"project.contributors.institution\")}\n",
    "Person Address\\t{g(\"project_contributors\", \"project.contributors.address\")}\n",
    "Person Roles\\t{g(\"project_contributors\", \"project.contributors.project_role.text\")}\n",
    "Protocol Type\\t{tab.join([protocol_type_map[pt] for pt, pd in protocol_map.items() for pn, p in pd.items()])}\n",
    "Protocol Name\\t{tab.join([p['id'] for pt, pd in protocol_map.items() for pn, p in pd.items()])}\n",
    "Protocol Description\\t{tab.join([p['description'] for pt, pd in protocol_map.items() for pn, p in pd.items()])}\n",
    "Protocol Hardware\\t{tab.join([p.get('hardware', '') for pt, pd in protocol_map.items() for pn, p in pd.items()])}\n",
    "Term Source Name\\tEFO\\tArrayExpress\n",
    "Term Source File\\thttp://www.ebi.ac.uk/efo/efo.owl\\thttp://www.ebi.ac.uk/arrayexpress/\n",
    "Comment[AEExperimentType]\\tRNA-seq of coding RNA from single cells\n",
    "Experimental Factor Name\\t{fill_this_label}\n",
    "Experimental Factor Type\\t{fill_this_label}\n",
    "Comment[EAAdditionalAttributes]\\t{fill_this_label}\n",
    "Comment[EACurator]\\t{tab.join(curators)}\n",
    "Comment[EAExpectedClusters]\\t\n",
    "Comment[ExpressionAtlasAccession]\\t{accession}\n",
    "Comment[HCALastUpdateDate]\\t{last_update_date}\n",
    "Comment[SecondaryAccession]\\t{project_uuid}\\t{tab.join(geo_accessions)}\n",
    "Comment[EAExperimentType]\\t{fill_this_label}\n",
    "SDRF File\\t{sdrf_file_name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{work_dir}/{idf_file_name}\", \"w\") as idf_file:\n",
    "    idf_file.write(idf_file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDRF File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HCA - SCEA column map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something has to be done for the case where a column is not there, spreadsheets are very random on that regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_columns = ['GENERIC_PROTOCOL_FIELD']\n",
    "big_table['UNDEFINED_FIELD'] = fill_this_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_map_chunks = [{\n",
    "    'Source Name': \"specimen_from_organism.biomaterial_core.biomaterial_id\",\n",
    "    'Characteristics[organism]': \"donor_organism.genus_species.ontology_label\",\n",
    "    'Characteristics[individual]': \"donor_organism.biomaterial_core.biomaterial_id\",\n",
    "    'Characteristics[sex]': \"donor_organism.sex\",\n",
    "    'Characteristics[age]': \"donor_organism.organism_age\",\n",
    "    'Unit [time unit]': \"donor_organism.organism_age_unit.text\",\n",
    "    'Characteristics[developmental stage]': \"donor_organism.development_stage.text\",\n",
    "    'Characteristics[organism part]': \"specimen_from_organism.organ.ontology_label\",\n",
    "    'Characteristics[sampling site]': \"specimen_from_organism.organ_parts.ontology_label\",\n",
    "    'Characteristics[cell type]': \"cell_suspension.selected_cell_types.ontology_label\",\n",
    "    'Characteristics[disease]': \"donor_organism.diseases.ontology_label\",\n",
    "    'Characteristics[organism status]': \"donor_organism.is_living\",\n",
    "#     'Characteristics[cause of death]': \"donor_organism.death.cause_of_death\",\n",
    "    'Characteristics[clinical history]': \"donor_organism.medical_history.test_results\",\n",
    "    'Comment[HCA bundle uuid]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[HCA bundle version]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[Sample_description]': \"specimen_from_organism.biomaterial_core.biomaterial_description\",\n",
    "    'Comment[biomaterial name]': \"specimen_from_organism.biomaterial_core.biomaterial_id\",\n",
    "    'Material Type': \"UNDEFINED_FIELD\",\n",
    "}, {\n",
    "    'Protocol REF': \"GENERIC_PROTOCOL_FIELD\",\n",
    "}, {\n",
    "    'Extract Name': \"specimen_from_organism.biomaterial_core.biomaterial_id\",\n",
    "    'Material Type': \"UNDEFINED_FIELD\",\n",
    "    'Comment[library construction]': \"library_preparation_protocol.library_construction_method.ontology_label\",\n",
    "    'Comment[input molecule]': \"library_preparation_protocol.input_nucleic_acid_molecule.ontology_label\",\n",
    "    'Comment[primer]': \"library_preparation_protocol.primer\",\n",
    "    'Comment[end bias]': \"library_preparation_protocol.end_bias\",\n",
    "    'Comment[umi barcode read]': \"library_preparation_protocol.umi_barcode.barcode_read\",\n",
    "    'Comment[umi barcode offset]': \"library_preparation_protocol.umi_barcode.barcode_offset\",\n",
    "    'Comment[umi barcode size]': \"library_preparation_protocol.umi_barcode.barcode_length\",\n",
    "    'Comment[cell barcode read]': \"library_preparation_protocol.cell_barcode.barcode_read\",\n",
    "    'Comment[cell barcode offset]': \"library_preparation_protocol.cell_barcode.barcode_offset\",\n",
    "    'Comment[cell barcode size]\",  ': \"library_preparation_protocol.cell_barcode.barcode_length\",\n",
    "    'Comment[sample barcode read]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[sample barcode offset]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[sample barcode size]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[single cell isolation]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[cDNA read]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[cDNA read offset]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[cDNA read size]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[LIBRARY_STRAND]': \"library_preparation_protocol.strand\",\n",
    "    'Comment[LIBRARY_LAYOUT]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[LIBRARY_SOURCE]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[LIBRARY_STRATEGY]': \"UNDEFINED_FIELD\",\n",
    "    'Comment[LIBRARY_SELECTION]': \"UNDEFINED_FIELD\"\n",
    "}, {\n",
    "    'Protocol REF': \"GENERIC_PROTOCOL_FIELD\",\n",
    "}, {\n",
    "    'Assay Name': \"specimen_from_organism.biomaterial_core.biomaterial_id\",\n",
    "    'Technology Type': \"UNDEFINED_FIELD\",\n",
    "    'Scan Name': \"specimen_from_organism.biomaterial_core.biomaterial_id\",\n",
    "    'Comment[RUN]': \"specimen_from_organism.biomaterial_core.biomaterial_id\",\n",
    "    'Comment[read1 file]': \"sequence_file.file_core.file_name_read1\",\n",
    "    'Comment[read2 file]': \"sequence_file.file_core.file_name_read2\",\n",
    "    'Comment[index1 file]': \"UNDEFINED_FIELD\",\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk 1: donor info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdrf_1 = pd.DataFrame({k: big_table[v] for k, v in convert_map_chunks[0].items() if v not in special_columns})\n",
    "sdrf_1 = sdrf_1.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixes for chunk 1\n",
    "1. Organism status: convert from 'is_alive' to 'status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdrf_1['Characteristics[organism status]'] = sdrf_1['Characteristics[organism status]'].apply(lambda x: 'alive' if x.lower() in ['yes', 'y'] else 'dead')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk 2: collection/dissociation/enrichment protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_term(term, name):\n",
    "    return map_proto_to_id(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_row(row):\n",
    "    return row.apply(lambda x: convert_term(x, row.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols_for_sdrf_2 = ['collection_protocol', 'dissociation_protocol', 'enrichment_protocol']\n",
    "\n",
    "sdrf_2 = big_table[[col for proto_type, col in protocol_columns if proto_type in protocols_for_sdrf_2]]\n",
    "sdrf_2 = sdrf_2.apply(convert_row)\n",
    "sdrf_2.columns = [\"Protocol REF\" for col in sdrf_2.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk 3: Library prep protocol info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdrf_3 = pd.DataFrame({k: big_table[v] for k, v in convert_map_chunks[2].items() if v not in special_columns})\n",
    "sdrf_3 = sdrf_3.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixes for chunk 3:\n",
    "1. Change \"Read n\" to \"readn\" in `['Comment[umi barcode read]', 'Comment[cell barcode read]']` columns\n",
    "2. In column `Comment[LIBRARY_STRAND]` add `\" strand\"` to the contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_map = {'': '', 'Read 1': \"read1\", 'Read 2': \"read2\"}\n",
    "\n",
    "sdrf_3[['Comment[umi barcode read]', 'Comment[cell barcode read]']] = sdrf_3[['Comment[umi barcode read]', 'Comment[cell barcode read]']].applymap(lambda x: read_map[x])\n",
    "sdrf_3['Comment[LIBRARY_STRAND]'] = sdrf_3['Comment[LIBRARY_STRAND]'] + \" strand\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk 4: Library preparation / sequencing protocol ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols_for_sdrf_4 = ['library_preparation_protocol', 'sequencing_protocol']\n",
    "\n",
    "sdrf_4 = big_table[[col for proto_type, col in protocol_columns if proto_type in protocols_for_sdrf_4]]\n",
    "sdrf_4 = sdrf_4.apply(convert_row)\n",
    "sdrf_4.columns = [\"Protocol REF\" for col in sdrf_4.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk 5: Sequence files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdrf_5 = pd.DataFrame({k: big_table[v] for k, v in convert_map_chunks[4].items() if v not in special_columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdrf = sdrf_1.join(sdrf_2).join(sdrf_3, rsuffix=\"_1\").join(sdrf_4, rsuffix=\"_1\").join(sdrf_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdrf = sdrf.rename(columns = {'Protocol REF_1' : \"Protocol REF\", 'Material Type_1': \"Material Type\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save SDRF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdrf.to_csv(f\"{work_dir}/{sdrf_file_name}\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
